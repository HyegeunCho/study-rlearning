{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "\n",
    "np.random.seed(1234)\n",
    "def randomize(): np.random.seed(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RND_MEAN = 0\n",
    "RND_STD = 0.0030\n",
    "\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험용 메인 함수\n",
    "def abalone_exec(epoch_count=10, mb_size=10, report=1):\n",
    "    load_abalone_dataset()\n",
    "    init_model()\n",
    "    train_and_test(epoch_count, mb_size, report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 적재 함수 정의\n",
    "def load_abalone_dataset():\n",
    "    with open('../../data/chap01/abalone.data.csv') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        next(csvreader, None) # 첫 행 스킵 (헤더)\n",
    "        rows = []\n",
    "        for row in csvreader:\n",
    "            rows.append(row)\n",
    "    \n",
    "    global data, input_cnt, output_cnt\n",
    "    input_cnt, output_cnt = 10, 1 # 퍼셉트론 입력, 출력의 크기 지정\n",
    "    data = np.zeros([len(rows), input_cnt + output_cnt])\n",
    "    \n",
    "    for n, row in enumerate(rows):\n",
    "        # 원핫 벡터 변환 처리 \n",
    "        if row[0] == 'I': data[n, 0] = 1\n",
    "        if row[0] == 'M': data[n, 1] = 1\n",
    "        if row[0] == 'F': data[n, 2] = 1\n",
    "        # 성별 이외의 정보를 일괄 복제 \n",
    "        data[n, 3:] = row[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 초기화 함수 정의\n",
    "# 퍼셉트론의 가중치 파라미터 weight와 bias를 초기화 \n",
    "def init_model():\n",
    "    # 전역 변수로 선언하여 다른 함수에서도 사용 가능 \n",
    "    # input_cnt, output_cnt는 load_abalone_dataset 함수에서 전역으로 선언되어 있어 바로 사용 가능 \n",
    "    global weight, bias, input_cnt, output_cnt\n",
    "    # 가중치 행렬 값을 정규 분포를 갖는 랜덤값으로 초기화 \n",
    "    weight = np.random.normal(RND_MEAN, RND_STD, [input_cnt, output_cnt])\n",
    "    # 초기에 지나친 영향을 끼치지 않도록 0으로 초기화 \n",
    "    bias = np.zeros([output_cnt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 평가 함수 정의\n",
    "def train_and_test(epoch_count, mb_size, report):\n",
    "    step_count = arrange_data(mb_size)\n",
    "    test_x, test_y = get_test_data()\n",
    "    \n",
    "    for epoch in range(epoch_count):\n",
    "        losses, accs = [], []\n",
    "        \n",
    "        for n in range(step_count):\n",
    "            train_x, train_y = get_train_data(mb_size, n)\n",
    "            loss, acc = run_train(train_x, train_y)\n",
    "            losses.append(loss)\n",
    "            accs.append(acc)\n",
    "            \n",
    "        if report > 0 and (epoch + 1) % report == 0:\n",
    "            acc = run_test(test_x, test_y)\n",
    "            print('Epoch {}: loss={:5.3f}, accuracy={:5.3f}/{:5.3f}'.\\\n",
    "                 format(epoch+1, np.mean(losses), np.mean(accs), acc))\n",
    "    \n",
    "    final_acc = run_test(test_x, test_y)\n",
    "    print('\\nFinal Test: final accuracy = {:5.3f}'.format(final_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 평가 데이터 획득 함수 정의\n",
    "def arrange_data(mb_size):\n",
    "    global data, shuffle_map, test_begin_idx\n",
    "    shuffle_map = np.arange(data.shape[0]) # 데이터의 수만큼 일련번호 발생\n",
    "    np.random.shuffle(shuffle_map) # 무작위로 순서 섞음\n",
    "    step_count = int(data.shape[0] * 0.8) // mb_size\n",
    "    test_begin_idx = step_count * mb_size\n",
    "    return step_count\n",
    "\n",
    "def get_test_data():\n",
    "    global adta, shuffle_map, test_begin_idx, output_cnt\n",
    "    test_data = data[shuffle_map[test_begin_idx:]]\n",
    "    return test_data[:, :-output_cnt], test_data[:, -output_cnt:]\n",
    "\n",
    "def get_train_data(mb_size, nth):\n",
    "    global data, shuffle_map, test_begin_idx, output_cnt\n",
    "    if nth == 0:\n",
    "        np.random.shuffle(shuffle_map[:test_begin_idx])\n",
    "    train_data = data[shuffle_map[mb_size * nth:mb_size*(nth+1)]]\n",
    "    return train_data[:, :-output_cnt], train_data[:, -output_cnt:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 실행함수와 평가 실행 함수 정의\n",
    "def run_train(x, y):\n",
    "    # 순전파 처리 \n",
    "    output, aux_nn = forward_neuralnet(x) # 단층 퍼셉트론 신경망에 대한 순전파 수행 => 입력 행렬 x로부터 신경망 출력 output 계산\n",
    "    loss, aux_pp = forward_postproc(output, y) # output과 y 로부터 손실함 수 loss 계산\n",
    "    # 신경망 계산 => 후처리 의 두가지 단계로 나누면 독립적으로 다른 형태로 교체하기 편리 \n",
    "    # aux_nn, aux_pp 값은 역전파에 사용되는 정보 \n",
    "    accuracy = eval_accuracy(output, y) # 정확도를 출력\n",
    "    \n",
    "    # 역전파 처리 \n",
    "    # 역전파 함수는 항상 순전파의 역순으로 실행되어야 함\n",
    "    # 순전파 때 출력이었던 성분의 손실 기울기를 내부 처리하여 순전파 때 입력이었던 성분의 손실 기울기를 반환해야 하기 때문\n",
    "    G_loss = 1.0 # 역전파의 시작점 \n",
    "    G_ouput = backprop_postproc(G_loss, aux_pp) # G_loss로부터 G_output을 구한다.\n",
    "    backprop_neuralnet(G_output, aux_nn)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def run_test(x, y):\n",
    "    output, _ = forward_neuralnet(x)\n",
    "    accuracy = eval_accuracy(output, y)\n",
    "    return accuracy\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
