{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 밴딧의 손잡이 목록을 작성한다.\n",
    "# 현재 손잡이 4 (인덱스는 3)가 가장 자주 양의 보상을 제공하도록 설정되어 있다.\n",
    "bandit_arms = [0.2, 0.0, -0.2, -2.0]\n",
    "num_arms = len(bandit_arms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullBandit(bandit):\n",
    "    # 랜덤한 값을 구한다\n",
    "    result = np.random.randn(1)\n",
    "    \n",
    "    if result > bandit:\n",
    "        # 양의 보상을 반환한다\n",
    "        return 1\n",
    "    else:\n",
    "        # 음의 보상을 반환한다\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 신경망 구현\n",
    "# 신경망은 각 밴딧 손잡이에 대한 일련의 값들로 구성\n",
    "# 각 값은 해당 밴딧을 선택할 때 반환되는 보상의 추정값을 의미\n",
    "# 정책 경사 방법을 이용해 선택된 액션에 대해 큰 보상을 받는 쪽으로 이동해나가며 에이전트를 업데이트\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 네트워크의 피드포워드 부분 구현\n",
    "weights = tf.Variable(tf.ones([num_arms]))\n",
    "# tf.ones([num_arms]) => 모든 요소가 1로 이루어진 shape가 4인 텐서 생성 [1, 1, 1, 1]\n",
    "output = tf.nn.softmax(weights)\n",
    "# weights의 값들의 비율을 총합이 1인 값으로 변환\n",
    "# 각 Arm을 선택할 확률이 저장됨.\n",
    "# 차후 학습을 통해 weights 값이 변경되고 그 결과가 tfModel이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.]\n",
      "[0.25 0.25 0.25 0.25]\n",
      "Selected Action: \n",
      "0.25\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 테스트용 셀 - 학습에 영향을 미치지 않음\n",
    "\n",
    "tRes = tf.Session().run(tf.ones([num_arms]))\n",
    "print(tRes)\n",
    "resOutput =tf.Session().run(tf.nn.softmax(tRes))\n",
    "print(resOutput)\n",
    "\n",
    "a = np.random.choice(resOutput, p=resOutput)\n",
    "print(\"Selected Action: \")\n",
    "print(a)\n",
    "b = np.argmax(actions == a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.argmax(x == 5): 2\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# 테스트용 셀 - 학습에 영향을 미치지 않음\n",
    "\n",
    "x = np.array([1, 3, 5, 4, 2, 6])\n",
    "print(f'np.argmax(x == 5): {np.argmax(x == 5)}')\n",
    "print(np.argmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 과정을 구현한다\n",
    "# 보상과 선택된 액션을 네트워크에 피드해줌으로써 비용을 계산하고 비용을 이용해 네트워크를 업데이트한다\n",
    "reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "\n",
    "responsible_output = tf.slice(output, action_holder, [1])\n",
    "# tf.slice는 다차원 배열 (또는 텐서)를 잘라내어 새로운 배열(텐서)를 생성하는 함수\n",
    "# 관련 내용은 https://pythonkim.tistory.com/65 여기 참조\n",
    "# 위 코드는 output에 저장된 텐서에서 action_holder에 저장된 특정 액션에 해당하는 값을 한개의 요소만 갖는 1차원 텐서로 뽑아내라는 뜻\n",
    "\n",
    "loss = -1 * (tf.log(responsible_output) * reward_holder)\n",
    "# 위 코드에서 reward_holder에는 responsible_output 값을 생성한 액션에 해당하는 보상값이 저장될 예정 (차후 돌아갈 sess 에서.)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에이전트를 학습시킬 총 에피소드의 수를 설정한다\n",
    "total_episodes = 1000\n",
    "# 밴딧 손잡이에 대한 점수판을 0으로 설정\n",
    "total_reward = np.zeros(num_arms)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Actions: [0.25 0.25 0.25 0.25]\n",
      "Selected Action: 0.25\n",
      "Updated Resp: [0.25]\n",
      "Updated weights: [1.001 0.999 0.999 0.999]\n",
      "\tRunning reward for the 4 arms of the bandit: [-19.  12.  70. 300.]\n",
      "Available Actions: [0.24686424 0.24919233 0.24810286 0.25584057]\n",
      "Selected Action: 0.24686424434185028\n",
      "Updated Resp: [0.24686424]\n",
      "Updated weights: [0.9862316  0.99624294 0.9919169  1.0234915 ]\n",
      "\tRunning reward for the 4 arms of the bandit: [-27.  12.  69. 315.]\n",
      "Available Actions: [0.24227208 0.24776036 0.2467943  0.2631732 ]\n",
      "Selected Action: 0.24227207899093628\n",
      "Updated Resp: [0.24227208]\n",
      "Updated weights: [0.96736145 0.9902406  0.9862419  1.0512754 ]\n",
      "\tRunning reward for the 4 arms of the bandit: [-30.  14.  71. 332.]\n",
      "Available Actions: [0.23837377 0.24564317 0.24602653 0.2699565 ]\n",
      "Selected Action: 0.24602653086185455\n",
      "Updated Resp: [0.24602653]\n",
      "Updated weights: [0.9506854  0.9812165  0.98353696 1.076016  ]\n",
      "\tRunning reward for the 4 arms of the bandit: [-32.  15.  78. 348.]\n",
      "Available Actions: [0.23306234 0.24326995 0.24838881 0.27527887]\n",
      "Selected Action: 0.23306234180927277\n",
      "Updated Resp: [0.23306234]\n",
      "Updated weights: [0.928372  0.9719254 0.9927355 1.0957536]\n",
      "\tRunning reward for the 4 arms of the bandit: [-38.  15.  83. 361.]\n",
      "Available Actions: [0.23075585 0.24129483 0.24910465 0.2788447 ]\n",
      "Selected Action: 0.24129483103752136\n",
      "Updated Resp: [0.24129483]\n",
      "Updated weights: [0.9190413 0.9636968 0.9958206 1.109021 ]\n",
      "\tRunning reward for the 4 arms of the bandit: [-37.  14.  88. 372.]\n",
      "Available Actions: [0.22962274 0.23739155 0.2494879  0.28349787]\n",
      "Selected Action: 0.24948790669441223\n",
      "Updated Resp: [0.2494879]\n",
      "Updated weights: [0.91452175 0.9479945  0.99801254 1.1259103 ]\n",
      "\tRunning reward for the 4 arms of the bandit: [-36.  11.  93. 383.]\n",
      "Available Actions: [0.2276715  0.2357607  0.25071204 0.2858557 ]\n",
      "Selected Action: 0.2858557105064392\n",
      "Updated Resp: [0.2858557]\n",
      "Updated weights: [0.9066978 0.9416529 1.0033329 1.134483 ]\n",
      "\tRunning reward for the 4 arms of the bandit: [-32.  15. 102. 394.]\n",
      "Available Actions: [0.2249119  0.23314606 0.2508293  0.29111278]\n",
      "Selected Action: 0.2911127805709839\n",
      "Updated Resp: [0.29111278]\n",
      "Updated weights: [0.89545035 0.931148   1.0048217  1.153809  ]\n",
      "\tRunning reward for the 4 arms of the bandit: [-33.  13. 107. 408.]\n",
      "Available Actions: [0.22242133 0.22862013 0.25272632 0.29623222]\n",
      "Selected Action: 0.222421333193779\n",
      "Updated Resp: [0.22242133]\n",
      "Updated weights: [0.88561535 0.9125752  1.0136102  1.1724455 ]\n",
      "\tRunning reward for the 4 arms of the bandit: [-34.   7. 114. 420.]\n",
      "Available Actions: [0.22087361 0.22541814 0.25245455 0.30125365]\n",
      "Selected Action: 0.252454549074173\n",
      "Updated Resp: [0.25245455]\n",
      "Updated weights: [0.8795717 0.8999233 1.0132607 1.1904656]\n",
      "\tRunning reward for the 4 arms of the bandit: [-33.   7. 117. 434.]\n",
      "Available Actions: [0.21766406 0.22558373 0.2505381  0.30621412]\n",
      "Selected Action: 0.2505381107330322\n",
      "Updated Resp: [0.2505381]\n",
      "Updated weights: [0.8659442  0.90155333 1.0065963  1.2078546 ]\n",
      "\tRunning reward for the 4 arms of the bandit: [-35.  11. 118. 447.]\n",
      "Available Actions: [0.21585703 0.22272094 0.24917348 0.31224853]\n",
      "Selected Action: 0.21585702896118164\n",
      "Updated Resp: [0.21585703]\n",
      "Updated weights: [0.85912156 0.89021945 1.0024064  1.2289271 ]\n",
      "\tRunning reward for the 4 arms of the bandit: [-32.  12. 122. 465.]\n",
      "Available Actions: [0.21529323 0.21751161 0.2465514  0.32064372]\n",
      "Selected Action: 0.32064372301101685\n",
      "Updated Resp: [0.32064372]\n",
      "Updated weights: [0.85846454 0.86845714 0.9938354  1.2578558 ]\n",
      "\tRunning reward for the 4 arms of the bandit: [-31.   4. 120. 482.]\n",
      "Available Actions: [0.21112101 0.21328716 0.2452531  0.33033872]\n",
      "Selected Action: 0.330338716506958\n",
      "Updated Resp: [0.33033872]\n",
      "Updated weights: [0.8415332  0.85216683 0.99204373 1.2898171 ]\n",
      "\tRunning reward for the 4 arms of the bandit: [-37.   1. 124. 497.]\n",
      "Available Actions: [0.20691685 0.21261626 0.24379346 0.33667335]\n",
      "Selected Action: 0.20691685378551483\n",
      "Updated Resp: [0.20691685]\n",
      "Updated weights: [0.8239307 0.8509785 0.9876524 1.3111107]\n",
      "\tRunning reward for the 4 arms of the bandit: [-37.   6. 127. 517.]\n",
      "Available Actions: [0.20462644 0.2080393  0.24080288 0.34653136]\n",
      "Selected Action: 0.24080288410186768\n",
      "Updated Resp: [0.24080288]\n",
      "Updated weights: [0.8157043 0.8321533 0.9785948 1.3433648]\n",
      "\tRunning reward for the 4 arms of the bandit: [-36.   3. 130. 542.]\n",
      "Available Actions: [0.20055744 0.20644407 0.23937689 0.3536216 ]\n",
      "Selected Action: 0.35362160205841064\n",
      "Updated Resp: [0.3536216]\n",
      "Updated weights: [0.7981823  0.8271905  0.97529197 1.3661227 ]\n",
      "\tRunning reward for the 4 arms of the bandit: [-39.   6. 134. 560.]\n",
      "Available Actions: [0.19817255 0.20455666 0.236558   0.3607128 ]\n",
      "Selected Action: 0.36071279644966125\n",
      "Updated Resp: [0.3607128]\n",
      "Updated weights: [0.78896296 0.8204433  0.9660099  1.3883462 ]\n",
      "\tRunning reward for the 4 arms of the bandit: [-38.   7. 135. 577.]\n",
      "Available Actions: [0.1946252  0.20427172 0.23491615 0.3661869 ]\n",
      "Selected Action: 0.20427171885967255\n",
      "Updated Resp: [0.20427172]\n",
      "Updated weights: [0.77297145 0.82114196 0.9612768  1.4055821 ]\n",
      "\tRunning reward for the 4 arms of the bandit: [-40.  12. 139. 594.]\n",
      "\n",
      "The agent thinks arm\n"
     ]
    }
   ],
   "source": [
    "# 텐서플로 그래프를 론칭한다\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        # 볼츠만 분포에 따라 액션 선택\n",
    "        actions = sess.run(output)\n",
    "        \n",
    "        a = np.random.choice(actions, p=actions)\n",
    "        # actions 배열 중에서 하나의 요소를 랜덤으로 고르는 함수, p 파라미터로 념겨준 겂은 각 요소를 선택하는 데 대한 확률 값\n",
    "        \n",
    "        action = np.argmax(actions == a)\n",
    "        # np.argmax(actions)는 actions 배열 중에서 가장 큰 값의 위치(인덱스 값)를 반환한다.\n",
    "        # np.argmax(actions == a) 는 actions 배열 중 a와 동일한 값의 위치를 반환 (요소 별 연산을 수행)\n",
    "        \n",
    "        # 위 로직을 수행하면, \n",
    "        # weight의 값을 총합 1인 값으로 정규화한 배열을 받아온다.\n",
    "        # 그리고 그 값들을 확률로 반영해서 랜덤으로 하나의 값을 선택\n",
    "        # 해당 값의 인덱스값 => Arm 번호를 받아온다.\n",
    "        \n",
    "        # 밴딧 손잡이 중 하나를 선택함으로써 보상을 받는다\n",
    "        reward = pullBandit(bandit_arms[action])\n",
    "        \n",
    "        # 네트워크를 업데이트한다\n",
    "        _, resp, ww = sess.run([update, responsible_output, weights], feed_dict={\n",
    "            reward_holder:[reward], \n",
    "            action_holder: [action]\n",
    "        })\n",
    "        \n",
    "        # 보상의 총계 업데이트\n",
    "        total_reward[action] += reward\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Available Actions: {actions}\")\n",
    "            print(f\"Selected Action: {a}\")\n",
    "            print(f\"Updated Resp: {resp}\")\n",
    "            print(f\"Updated weights: {ww}\")\n",
    "            print(f\"\\tRunning reward for the {num_arms} arms of the bandit: {total_reward}\")\n",
    "        i += 1\n",
    "\n",
    "print(\"\\nThe agent thinks arm\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
